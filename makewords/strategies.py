'''
strategies.py

strategies here are functions that take a list of candidate word tripples
such as ('rand', 'dom', 'inate') from the two words 'random' and 'dominate',
scores them and returns the top n results.
'''
from __future__ import print_function
from __future__ import division
from utils import *

#### strategy 1
# assume that good words are simply one that have a particular ratio when
# they overlap.
def strategy1(candidates, n):
    "return the top scoring candidates via portman_score"
    return best_n(candidates, norvig_score, n)

# approach given by Peter norvig in design of computer programs course.
def norvig_score(triple):
    "Return the numeric score for a (start, mid, end) triple."
    S, M, E = map(len, triple)
    T = S + M + E
    return T - abs(S-T/4.) - abs(M-T/2.) - abs(E-T/4.)


#### strategy 2
# we only want orignal words really so remove any ones that are not.
# use the norvig score still though.
@debug_print(norvig_score)
def strategy2(candidates, n):
    candidates = set(candidates)
    print("initial candidate words:", len(candidates))
    candidates = set(filter(original, candidates))
    print("orignal candidate words:", len(candidates))
    return best_n(candidates, norvig_score, n)

# result: removed 2556 non original words from the list.
# initial candidate words: 249417
# original candidate words: 246861

def original(candidate):
    "a word is original if its not in our real words list"
    return cat(candidate) not in real_words


# add web2a for conjunctive words like fin-shaped
unix_web2a_path = '/usr/share/dict/web2a'
unix_words_path = '/usr/share/dict/words'

unix_words  = set(open(unix_words_path).read().split())
web2a_words = set(open(unix_words_path).read().replace("-", "").replace(" ", "").split("\n"))

# we will include hyponated words with the hyphons removed also.
real_words = unix_words | web2a_words
cat = ''.join


#### strategy 3
# darius bacon suggests removing words with specific noise characters from
# the list. instead of doing this completely which might be a bit aggressive
# we can just make it so that these words just score lower in our rankings.

def punish_noise(triple):
    "Return the numeric score for a (start, mid, end) triple."
    w0 = norvig_score(triple)
    w1 = noise_weight(triple) # 0.00 - 1.00
    return w0 * w1

@debug_print(punish_noise)
def strategy3(candidates, n):
    candidates = filter(original, candidates)
    return best_n(set(candidates), punish_noise, n)

# this is slows down the program a lot but is should only be something we do once.
# configuring the program differently could also speed this up.
def noise_weight(tripple, z1=1, z2=20):
    word = cat(tripple)
    penatly = 1
    for front in left_noise:
        if word.startswith(front):
            penatly += z1
    for end in right_noise:
        if word.endswith(end):
            penatly += z2
    return 1 / penatly

# from: https://github.com/darius/languagetoys/blob/master/portmanteau.py
left_noise = '''
be bi em en di duo im iso non oct octo out pre quad quadra quadri re
sub tri un uni
'''.split()

right_noise = '''
ability able adian age an ation d ed en ent er es escent ful ian ic
ies ily iness ing ish ite ize less let log like liness ly ness og
ogy proof r ress ry s ship tion y
'''.split()


#### Strategy 4
# borrow again from darius bacon and see how the word probabilities do using
# trigrams.
import pmodel

tri = pmodel.load_model("data/trigrams")

def trigram_pw_score(triple):
    word = cat(triple)
    s, M, e = map(len, triple)
    T = s + M + e
    w0 = (tri.log10_pword(word) * 16**(-T/M)) / T**3
    w1 = noise_weight(triple) # 0.00 - 1.00
    return w0 * w1

@debug_print(trigram_pw_score)
def strategy4(candidates, n):
    candidates = filter(original, candidates)
    return best_n(set(candidates), trigram_pw_score, n)

# from: https://github.com/darius/languagetoys/blob/master/portmanteau.py
def bacon_score(triple):
    """Return a measure of the quality or interestingness of a
    portmanteau. Lower scores are better. A 'good' portmanteau is made
    out of common words with a lot of overlap between them."""
    S, M, E = triple
    L = len(S) + len(M) - len(E)
    return -math.log10(pdist.Pw(S+M) * pdist.Pw(M+E) * 16**(-float(L)/len(affix)))

#### Strategy 5
# throw some more together in order make the different scoring tactics
# interact better maybe a better approach is to create a set of features that we
# can play around with weights and see what produces the best results.

# possible features:
#   feature        notes
#   =================================================
#   total_len
#   start_len
#   middle_len
#   end_len
#   norvig_score   tries to find optional ratio
#   noise_score    the lower the better here.
#   log10_pword    probablity of the word generated by trigram chunks
#   bacon_score    probabilty of composing words (need a dictinary for this) and amount overlap
